{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirleyzz/cap5610_machineleanrning/blob/master/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c8XBE9ujESWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d949f077-2363-425d-d9b6-708aadb94160"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import keras\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OJ9PuoqnEUKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load the training and test data using keras\n",
        "(x_train_origin, y_train_origin),(x_test_origin, y_test_origin) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3hQXx-A-EVnj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train_origin.reshape((60000, 28 * 28))\n",
        "x_train = x_train.astype('float32') / 255\n",
        "\n",
        "x_test = x_test_origin.reshape((10000, 28 * 28))\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1PBFyD5EW8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#split into 10 classes with categorial labels\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "y_train = to_categorical(y_train_origin, num_classes=10) \n",
        "y_test = to_categorical(y_test_origin, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1R4MxVREYUe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculate cross entropy\n",
        "def compute_scores(w,b,X):\n",
        "  return np.dot(X,w.T) + b\n",
        "def softmax(scores):\n",
        "  exp = np.exp(scores)\n",
        "  sum_exp = np.sum(exp, axis=1,keepdims=True)\n",
        "  softmax = exp / sum_exp\n",
        "  return softmax\n",
        "def cross_entropy(X, Y, scores):\n",
        "  m = X.shape[0]\n",
        "  loss = - (1 / m) * np.sum(Y * np.log(scores))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikxha7u_EaFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize w and b\n",
        "def initialize_params_with_zeros(n_classes, n_features):\n",
        "  w = np.zeros((n_classes, n_features))\n",
        "  b = np.zeros((1, n_classes))\n",
        "  return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zl7Wze5UEbhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict\n",
        "def predict(w,b,X):\n",
        "  scores = compute_scores(w,b,X)\n",
        "  probs = softmax(scores)\n",
        "  preds = np.argmax(probs,axis=1)\n",
        "  print(preds)\n",
        "  #return np.argmax(probs, axis=1)[:, np.newaxis]\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bx7xcbejEcys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training process\n",
        "def train(X_train, Y_train, n_iters=10, learning_rate=0.01,batch = 100, n_classes = 10):\n",
        "  n_samples,n_features = X_train.shape\n",
        "  w, b = initialize_params_with_zeros(n_classes,n_features)\n",
        "  #print(w.shape)\n",
        "  all_losses = []\n",
        "  #stochastic gradient regression\n",
        "  for item in range(n_iters):\n",
        "    shuffled_indices = np.random.permutation(n_samples)\n",
        "    X_shuffled = X_train[shuffled_indices,:]\n",
        "    Y_shuffled = Y_train[shuffled_indices,:]\n",
        "    \n",
        "    for i in range(0, n_samples, batch):\n",
        "\n",
        "      X_i = X_shuffled[i:i+batch,:]\n",
        "      Y_i = Y_shuffled[i:i+batch,:]\n",
        "      scores = compute_scores(w,b,X_i)\n",
        "      probs = softmax(scores)\n",
        "      loss = cross_entropy(X_i, Y_i, probs)\n",
        "      #print(probs.shape)\n",
        "      dw = -1. / n_samples * np.dot((Y_i - probs).T,X_i)\n",
        "      db = -1. / n_samples * np.sum(Y_i - probs)\n",
        "      w = w - learning_rate * dw\n",
        "      b = b - learning_rate * db\n",
        "    all_losses.append(loss)\n",
        "    \n",
        "    print(f'Iteration number: {item}, loss: {np.round(loss, 4)}')\n",
        "  return w, b, all_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7f2tfe0KEhh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iters, learning_rate,batch,n_classes):\n",
        "  w, b, loss = train(X_train, Y_train, num_iters, learning_rate, batch,n_classes)\n",
        "  #predict test/train set examples \n",
        "  Y_pred_test = predict(w, b, X_test)\n",
        "  Y_pred_train = predict(w, b, X_train)\n",
        "  #use argmax to return indices of the maximum values along an axis\n",
        "  Y_true_test = np.argmax(Y_test, axis=1)\n",
        "  Y_true_train = np.argmax(Y_train,axis=1)\n",
        "                         \n",
        "  #print train/test Errors\n",
        "  print(\"\")\n",
        "  print(\"train accuracy: {} %\".format(np.sum(Y_pred_train == Y_true_train)/len(Y_pred_train) * 100))\n",
        "  print(\"test accuracy: {} %\".format(np.sum(Y_pred_test == Y_true_test)/len(Y_pred_test) * 100))\n",
        " \n",
        "  d = {\"costs\": loss, \"Y_pred_test\": Y_pred_test, \n",
        "        \"Y_pred_train\" : Y_pred_train, \n",
        "        \"w\" : w, \n",
        "        \"b\" : b,\n",
        "        \"learning_rate\" : learning_rate,\n",
        "        \"num_iters\": num_iters}\n",
        "    \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qe-3Z1G3EkEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "fd89cadb-3ef4-4e27-8726-b3e4d93fc47c"
      },
      "cell_type": "code",
      "source": [
        "d = model(x_train, y_train, x_test, y_test, num_iters=10, learning_rate=0.01,batch=1000, n_classes=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 0, loss: 2.2919\n",
            "Iteration number: 1, loss: 2.2824\n",
            "Iteration number: 2, loss: 2.2696\n",
            "Iteration number: 3, loss: 2.2601\n",
            "Iteration number: 4, loss: 2.2491\n",
            "Iteration number: 5, loss: 2.2378\n",
            "Iteration number: 6, loss: 2.2289\n",
            "Iteration number: 7, loss: 2.2169\n",
            "Iteration number: 8, loss: 2.2103\n",
            "Iteration number: 9, loss: 2.1974\n",
            "[7 2 1 ... 8 8 6]\n",
            "[3 0 4 ... 8 0 8]\n",
            "\n",
            "train accuracy: 70.155 %\n",
            "test accuracy: 70.47 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wNo-76wZEl5n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}