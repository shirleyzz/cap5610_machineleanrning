{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirleyzz/cap5610_machineleanrning/blob/master/Copy_of_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YxtoTwJz71fi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import keras\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKebYiAev5mF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load the training and test data using keras\n",
        "(x_train_origin, y_train_origin),(x_test_origin, y_test_origin) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVWJ8ysK-CmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train_origin.reshape((60000, 28 * 28))\n",
        "x_train = x_train_origin.astype('float32') / 255\n",
        "\n",
        "x_test = x_test_origin.reshape((10000, 28 * 28))\n",
        "x_test = x_test_origin.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ux2HdzI2GLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#split into 10 classes with categorial labels\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "y_train = to_categorical(y_train_origin, num_classes=10) \n",
        "y_test = to_categorical(y_test_origin, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlFgDMdB-sHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So each of the ten classifiers has an input layer consisting of 28 x 28 input neurons and an output layer consisting of a single output neuron.\n"
      ]
    },
    {
      "metadata": {
        "id": "LVDgI6FHHokM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculate cross entropy\n",
        "def compute_scores(w,b,X):\n",
        "  return np.dot(X,w.T) + b\n",
        "def softmax(scores):\n",
        "  exp = np.exp(scores)\n",
        "  sum_exp = np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "  softmax = exp / sum_exp\n",
        "  return softmax\n",
        "def cross_entropy(X, Y, scores):\n",
        "  m = X.shape[0]\n",
        "  loss = - (1 / m) * np.sum(Y * np.log(scores))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPHrzJsge085",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tranforms vector Y of labels to one-hot encoded matrix\n",
        "def one_hot(X, Y, n_classes=10):\n",
        "  m = X.shape[0]\n",
        "  one_hot = np.zeros((m, n_classes))\n",
        "  one_hot[np.arange(m), Y.T] = 1\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THW9q3YPc61M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize w and b\n",
        "def initialize_params_with_zeros(n_classes, n_features):\n",
        "  w = np.random.rand(n_classes, n_features)\n",
        "  b = np.zeros((1, n_classes))\n",
        "  return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXSC79lMdoRk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict\n",
        "def predict(w,b,X):\n",
        "  scores = compute_scores(w,b,X)\n",
        "  probs = softmax(scores)\n",
        "  preds = np.argmax(probs,axis=1)\n",
        "  print(preds)\n",
        "  #return np.argmax(probs, axis=1)[:, np.newaxis]\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFkjcXsbgZie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training process\n",
        "def train(X_train, Y_train, n_iters=10, learning_rate=0.01,n_classes = 10):\n",
        "  n_samples,n_features = X_train.shape\n",
        "  w, b = initialize_params_with_zeros(n_classes,n_features)\n",
        "  #print(w.shape)\n",
        "  all_losses = []\n",
        "  #stochastic gradient regression\n",
        "  for item in range(n_iters):\n",
        "    cost = 0.0\n",
        "    for i in range(n_samples):\n",
        "      rand_ind = np.random.randint(0,n_samples)\n",
        "      X_i = X_train[rand_ind,:].reshape(1,n_features)\n",
        "      Y_i = Y_train[rand_ind].reshape(1,1)\n",
        "      y_one_hot = one_hot(X_i,Y_i)\n",
        "      scores = compute_scores(w,b,X_i)\n",
        "      probs = softmax(scores)\n",
        "      loss = cross_entropy(X_i,y_one_hot, probs)\n",
        "      #print(probs.shape)\n",
        "      dw = (1. / n_samples) * np.dot((probs - y_one_hot).T,X_i)\n",
        "      db = (1. / n_samples) * np.sum(probs - y_one_hot, axis=0)\n",
        "      w = w - learning_rate * dw\n",
        "      b = b - learning_rate * db\n",
        "    all_losses.append(loss)\n",
        "    print(f'Iteration number: {item}, loss: {np.round(loss, 4)}')\n",
        "  return w, b, all_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dkyRZGpS5mza",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iters, learning_rate,n_classes):\n",
        "  w, b, loss = train(X_train, Y_train, num_iters, learning_rate, n_classes)\n",
        "  #predict test/train set examples \n",
        "  Y_pred_test = predict(w, b, X_test)\n",
        "  Y_pred_train = predict(w, b, X_train)\n",
        "  #use argmax to return indices of the maximum values along an axis\n",
        "  Y_true_test = np.argmax(Y_test, axis=0)\n",
        "  Y_true_train = np.argmax(Y_train,axis=0)\n",
        "                         \n",
        "\n",
        "  #print train/test Errors\n",
        "  print(\"\")\n",
        "  print(\"train accuracy: {} %\".format(100*sum(Y_pred_train==Y_true_train)/(float(len(Y_train)))))\n",
        "  print(\"test accuracy: {} %\".format(100*sum(Y_pred_test==Y_true_test)/(float(len(Y_test)))))\n",
        " \n",
        "  d = {\"costs\": loss, \"Y_pred_test\": Y_pred_test, \n",
        "        \"Y_pred_train\" : Y_pred_train, \n",
        "        \"w\" : w, \n",
        "        \"b\" : b,\n",
        "        \"learning_rate\" : learning_rate,\n",
        "        \"num_iters\": num_iters}\n",
        "    \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "smB0hnnT1moN",
        "colab_type": "code",
        "outputId": "8616d01a-52d0-4d94-e525-768c6c04914a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(x_train, y_train_origin, x_test, y_test_origin, num_iters=10, learning_rate=0.01,n_classes=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 0, loss: 6.077\n",
            "Iteration number: 1, loss: 0.6346\n",
            "Iteration number: 2, loss: 7.5571\n",
            "Iteration number: 3, loss: 5.7708\n",
            "Iteration number: 4, loss: 3.2331\n",
            "Iteration number: 5, loss: 2.9359\n",
            "Iteration number: 6, loss: 4.2981\n",
            "Iteration number: 7, loss: 0.4773\n",
            "Iteration number: 8, loss: 3.8013\n",
            "Iteration number: 9, loss: 4.2569\n",
            "[1 8 7 ... 1 2 4]\n",
            "[7 4 7 ... 1 4 2]\n",
            "\n",
            "train accuracy: 7.946666666666666 %\n",
            "test accuracy: 16.57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsWFAbpYeGEd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}