{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirleyzz/cap5610_machineleanrning/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NsL7gvINn_js",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea76f14d-e1b1-4ca8-df53-481ebd24d6ec"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # Simple implementation of the sigmoid function for vectors.\n",
        "    return np.vectorize(sigmoid_double)(z)\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    # Layers are stacked to build a sequential neural network.\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "\n",
        "        # A layer know its predecessor (previous layer)\n",
        "        self.previous = None\n",
        "        # and its successor (next layer).\n",
        "        self.next = None\n",
        "\n",
        "        # Each layer can persist data flowing into and out of it\n",
        "        # in the forward pass.\n",
        "        self.input_data = None\n",
        "        self.output_data = None\n",
        "\n",
        "        # Analogously, a layer holds input and output data\n",
        "        # for the backward pass.\n",
        "        self.input_delta = None\n",
        "        self.output_delta = None\n",
        "\n",
        "    def connect(self, layer):\n",
        "        # This method connects a layer to its direct neighbors\n",
        "        # in the sequential network.\n",
        "        self.previous = layer\n",
        "        layer.next = self\n",
        "\n",
        "    def forward(self):\n",
        "        # Each layer implementation has to provide a function\n",
        "        # to feed input data forward.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_forward_input(self):\n",
        "        # input_data is reserved for the first layer;\n",
        "        # all others get their inputs from the predecessors.\n",
        "        if self.previous is not None:\n",
        "            return self.previous.output_data\n",
        "        else:\n",
        "            return self.input_data\n",
        "\n",
        "    def backward(self):\n",
        "        # Layers have to implement backpropagation of error terms -\n",
        "        # a way to feed input errors backward through the network.\n",
        "        raise NotImplemented\n",
        "\n",
        "    def get_backward_input(self):\n",
        "        # input_delta is reserved for the last layer;\n",
        "        # all other layers get their error terms from their successors.\n",
        "        if self.next is not None:\n",
        "            return self.next.output_delta\n",
        "        else:\n",
        "            return self.input_delta\n",
        "\n",
        "    def clear_deltas(self):\n",
        "        # You compute and accumulate deltas per mini-batch,\n",
        "        # after which you need to reset these deltas.\n",
        "        pass\n",
        "\n",
        "    def update_params(self, learning_rate):\n",
        "        # Update layer parameters according to current deltas,\n",
        "        # using the specified learning_rate.\n",
        "        pass\n",
        "\n",
        "    def describe(self):\n",
        "        # Layer implementation can print their properties.\n",
        "        raise NotImplemented\n",
        "\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "    # This activation layer uses the sigmoid function\n",
        "    # to activate neurons.\n",
        "    def __init__(self, input_dim):\n",
        "        super(ActivationLayer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = input_dim\n",
        "\n",
        "    def forward(self):\n",
        "        # The forward pass is simply applying\n",
        "        # the sigmoid function to the input_data.\n",
        "        data = self.get_forward_input()\n",
        "        self.output_data = sigmoid(data)\n",
        "        #print(\"activation layer\")\n",
        "        #print(\"  data:\", data.shape)\n",
        "        #print(\"  output_data:\", self.output_data.shape)\n",
        "\n",
        "    def backward(self):\n",
        "        # The backward pass is element-wise multiplication of\n",
        "        # the error term with the sigmoid derivative evaluated at\n",
        "        # the input to this layer.\n",
        "        delta = self.get_backward_input()\n",
        "        data = self.get_forward_input()\n",
        "        self.output_delta = delta * sigmoid_prime(data)\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"|-- \" + self.__class__.__name__)\n",
        "        print(\"  |-- dimensions: ({} {})\".format(self.input_dim, self.output_dim))\n",
        "\n",
        "\n",
        "class DenseLayer(Layer):\n",
        "    # Dense layers have input and output dimensions.\n",
        "    def __init__(self,  input_dim, output_dim):\n",
        "\n",
        "        super(DenseLayer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Randomly initialize weight matrix and bias vector.\n",
        "        self.weight = np.random.randn(output_dim, input_dim)\n",
        "        self.bias = np.random.randn(output_dim, 1)\n",
        "\n",
        "        # The layer parameters consist of weights and bias terms.\n",
        "        self.params = [self.weight, self.bias]\n",
        "\n",
        "        # Deltas for weights and biases are set to 0.\n",
        "        self.delta_w = np.zeros(self.weight.shape)\n",
        "        self.delta_b = np.zeros(self.bias.shape)\n",
        "\n",
        "    def forward(self):\n",
        "        # The forward pass of the dense layer is\n",
        "        # the affine-linear transformation on the input data\n",
        "        # described by weights and biases.\n",
        "        data = self.get_forward_input()\n",
        "        self.output_data = np.dot(self.weight, data) + self.bias\n",
        "        #print(\"dense layer\")\n",
        "        #print(\"  data:\", data.shape)\n",
        "        #print(\"  weight:\", self.weight.shape)\n",
        "        #print(\"  bias:\", self.bias.shape)\n",
        "        #print(\"  output_data:\", self.output_data.shape)\n",
        "\n",
        "    def backward(self):\n",
        "        # For the backward pass, you first get input data and delta.\n",
        "        data = self.get_forward_input()\n",
        "        delta = self.get_backward_input()\n",
        "\n",
        "        # The current delta is added to the bias delta.\n",
        "        self.delta_b += delta\n",
        "\n",
        "        # Then you add this term to the weight delta.\n",
        "        self.delta_w += np.dot(delta, data.transpose())\n",
        "\n",
        "        # The backward pass is completed by passing\n",
        "        # an output delta to the previous layer.\n",
        "        self.output_delta = np.dot(self.weight.transpose(), delta)\n",
        "\n",
        "    def update_params(self, rate):\n",
        "        # Using weight and bias deltas,\n",
        "        # you can update model parameters\n",
        "        # with gradient descent.\n",
        "        self.weight -= rate * self.delta_w\n",
        "        self.bias -= rate * self.delta_b\n",
        "\n",
        "    def clear_deltas(self):\n",
        "        # After updating parameters,\n",
        "        # you should reset all deltas.\n",
        "        self.delta_w = np.zeros(self.delta_w.shape)\n",
        "        self.delta_b = np.zeros(self.delta_b.shape)\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"|--- \" + self.__class__.__name__)\n",
        "        print(\"  |-- dimensions: ({}, {})\".format(self.input_dim, self.output_dim))\n",
        "\n",
        "\n",
        "class MSE:\n",
        "    # Mean squared error loss function\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_function(predictions, labels):\n",
        "        diff = predictions - labels\n",
        "        # By defining MSE as 0.5 times\n",
        "        # the square difference between\n",
        "        # predictions and labels ...\n",
        "        return 0.5 * sum(diff * diff)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_derivative(predictions, labels):\n",
        "        # ... the loss derivative is simply\n",
        "        # the difference between predictions and labels\n",
        "        #print(\"predictions:\", predictions)\n",
        "        #print(\"labels:\", labels)\n",
        "        return predictions - labels\n",
        "\n",
        "\n",
        "class SequentialNetwork:\n",
        "    # In a sequential neural network, you stack layers sequentially.\n",
        "    def __init__(self, loss=None):\n",
        "        print(\"Initialize Network...\")\n",
        "        self.layers = []\n",
        "        # If no loss is provided, MSE is used.\n",
        "        if loss is None:\n",
        "            self.loss = MSE()\n",
        "\n",
        "    def add(self, layer):\n",
        "        # Whenever you add a layer, you connect it\n",
        "        # to its predecessor and let it describe itself.\n",
        "        self.layers.append(layer)\n",
        "        layer.describe()\n",
        "        if len(self.layers) > 1:\n",
        "            self.layers[-1].connect(self.layers[-2])\n",
        "\n",
        "    def train(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):\n",
        "        n = len(training_data)\n",
        "        # To train you network, you pass over the data\n",
        "        # for as many times as there are epochs.\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k + mini_batch_size] for\n",
        "                k in range(0, n, mini_batch_size)\n",
        "            ]\n",
        "            # For each mini batch, you train your network.\n",
        "            for mini_batch in mini_batches:\n",
        "                self.train_batch(mini_batch, learning_rate)\n",
        "            if test_data:\n",
        "                # If you provided test data,\n",
        "                # you evaluate your network on it after each epoch.\n",
        "                n_test = len(test_data)\n",
        "                print(\"Epoch {0}: {1} / {2}\".format(epoch, self.evaluate(test_data), n_test))\n",
        "            else:\n",
        "                print(\"Epoch {0} complete\".format(epoch))\n",
        "\n",
        "    def train_batch(self, mini_batch, learning_rate):\n",
        "        # To train the network you,\n",
        "        # compute feed-forward and backward pass...\n",
        "        self.forward_backward(mini_batch)\n",
        "        # ... and then update model parameters accordingly.\n",
        "        self.update(mini_batch, learning_rate)\n",
        "\n",
        "    def update(self, mini_batch, learning_rate):\n",
        "        # A common technique is to normalize\n",
        "        # the learning rate by the mini-batch size.\n",
        "        learning_rate = learning_rate / len(mini_batch)\n",
        "\n",
        "        # Update all layers\n",
        "        for layer in self.layers:\n",
        "            layer.update_params(learning_rate)\n",
        "        # Clear all deltas in each layer.\n",
        "        for layer in self.layers:\n",
        "            layer.clear_deltas()\n",
        "\n",
        "    def forward_backward(self, mini_batch):\n",
        "        for x, y in mini_batch:\n",
        "            self.layers[0].input_data = x\n",
        "            # For each sample in the mini batch,\n",
        "            # feed the features forward layer by layer.\n",
        "            for layer in self.layers:\n",
        "                layer.forward()\n",
        "            # Compute the loss derivative for the output data.\n",
        "            self.layers[-1].input_delta = \\\n",
        "                self.loss.loss_derivative(self.layers[-1].output_data, y)\n",
        "            # Do layer-by-layer backpropagation of error terms.\n",
        "            for layer in reversed(self.layers):\n",
        "                layer.backward()\n",
        "\n",
        "    def single_forward(self, x):\n",
        "        # Pass a single sample forward and\n",
        "        # return result.\n",
        "        self.layers[0].input_data = x\n",
        "        for layer in self.layers:\n",
        "            layer.forward()\n",
        "        return self.layers[-1].output_data\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # Compute accuracy on test data.\n",
        "        test_results = [(\n",
        "            np.argmax(self.single_forward(x)),\n",
        "            np.argmax(y)\n",
        "        ) for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oijvip_oronJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7401efd2-f976-4b59-98ca-f24dc311e7ea"
      },
      "cell_type": "code",
      "source": [
        "# load train and test data\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()\n",
        "\n",
        "# preprocess data\n",
        "\n",
        "# this is a big mess\n",
        "# need to clean up\n",
        "\n",
        "train_images = train_images_original.reshape((60000, 28 * 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "train_labels = train_labels.reshape((60000, 10, 1))\n",
        "\n",
        "test_labels = to_categorical(test_labels_original)\n",
        "test_labels = test_labels.reshape(10000, 10, 1)\n",
        "\n",
        "train_data = [(train_images[i], train_labels[i]) for i in range(0, 60000)]\n",
        "test_data = [(test_images[i], test_labels[i]) for i in range(0, 10000)]\n",
        "\n",
        "#print(train_images.shape)\n",
        "#print(train_labels.shape)\n",
        "\n",
        "#print(test_images.shape)\n",
        "#print(test_labels.shape)\n",
        "\n",
        "net = SequentialNetwork()\n",
        "net.add(DenseLayer(784, 100))\n",
        "net.add(ActivationLayer(100))\n",
        "net.add(DenseLayer(100, 10))\n",
        "net.add(ActivationLayer(10))\n",
        "\n",
        "#net.add(DenseLayer(392, 196))\n",
        "#net.add(ActivationLayer(196))\n",
        "#net.add(DenseLayer(196, 10))\n",
        "#net.add(ActivationLayer(10))\n",
        "\n",
        "net.train(train_data,\n",
        "          epochs=10,\n",
        "          mini_batch_size=10,\n",
        "          learning_rate=3.0,\n",
        "          test_data=test_data)\n",
        "\n",
        "#print(net.single_forward(test_data[0][0]))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "Initialize Network...\n",
            "|--- DenseLayer\n",
            "  |-- dimensions: (784, 100)\n",
            "|-- ActivationLayer\n",
            "  |-- dimensions: (100 100)\n",
            "|--- DenseLayer\n",
            "  |-- dimensions: (100, 10)\n",
            "|-- ActivationLayer\n",
            "  |-- dimensions: (10 10)\n",
            "Epoch 0: 7662 / 10000\n",
            "Epoch 1: 7744 / 10000\n",
            "Epoch 2: 8420 / 10000\n",
            "Epoch 3: 8580 / 10000\n",
            "Epoch 4: 8627 / 10000\n",
            "Epoch 5: 8666 / 10000\n",
            "Epoch 6: 8698 / 10000\n",
            "Epoch 7: 8728 / 10000\n",
            "Epoch 8: 8784 / 10000\n",
            "Epoch 9: 8823 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EYkVgzE2sJyO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}